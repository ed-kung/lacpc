{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00d5cea8-49ab-4fa8-9c6e-91417d1b4c4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '../../intermediate_data/subsection_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 106\u001b[0m\n\u001b[0;32m    103\u001b[0m subsection_data_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(subsection_data)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Save subsection data to Week 6 directory\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m \u001b[43msubsection_data_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubsection_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Print the DataFrame\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(subsection_data_df)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '../../intermediate_data/subsection_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Path to the HTML file\n",
    "file_path = '../../raw_data/lamunicipalcode.html'\n",
    "\n",
    "# Open and read the HTML file\n",
    "with open(file_path, 'r', encoding='utf-8') as municipal_code:\n",
    "    content = municipal_code.read()\n",
    "\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "# Find chapters\n",
    "chapters = soup.find_all('div', class_='rbox Chapter')\n",
    "chapter_data = []\n",
    "for chapter in chapters:\n",
    "    a_tag = chapter.find('a')\n",
    "    if a_tag:\n",
    "        chapter_id = a_tag.attrs['id']\n",
    "    chapter_text = chapter.get_text(separator=\" | \").strip()\n",
    "    \n",
    "    chapter_data.append({\n",
    "        'Chapter ID': chapter_id,\n",
    "        'Chapter Text': chapter_text\n",
    "    })\n",
    "\n",
    "chapter_data = pd.DataFrame.from_dict(chapter_data)\n",
    "\n",
    "# Save chapter data to Week 6 directory\n",
    "output_directory = '../../intermediate_data/'\n",
    "chapter_data.to_csv(output_directory + \"chapter_data.csv\", header=True, index=False)\n",
    "\n",
    "# Find articles\n",
    "articles = soup.find_all('div', class_='Article toc-destination rbox')\n",
    "article_data = []\n",
    "for article in articles:\n",
    "    a_tag = article.find('a')\n",
    "    if a_tag:\n",
    "        article_id = a_tag.attrs['id']\n",
    "    article_text = article.get_text(separator=\" | \")\n",
    "    article_data.append({\n",
    "        'Article ID': article_id,\n",
    "        'Article Text': article_text\n",
    "    })\n",
    "\n",
    "article_data = pd.DataFrame.from_dict(article_data)\n",
    "\n",
    "# Save article data to Week 6 directory\n",
    "article_data.to_csv(output_directory + \"article_data.csv\", header=True, index=False)\n",
    "\n",
    "# Find Sections\n",
    "sections = soup.find_all('div', class_='Section toc-destination rbox')\n",
    "section_data = []\n",
    "for section in sections:\n",
    "    a_tag = section.find('a')\n",
    "    if a_tag:\n",
    "        section_id = a_tag.attrs['id']\n",
    "    section_text = section.get_text(separator=\" | \")  #i saw the difference that adding the separator makes, why is that\n",
    "    section_data.append({\n",
    "        'Section ID': section_id,\n",
    "        'Section Text': section_text\n",
    "    })\n",
    "    \n",
    "section_data = pd.DataFrame.from_dict(section_data)\n",
    "\n",
    "# Save section data to Week 6 directory\n",
    "section_data.to_csv(output_directory + \"section_data.csv\", header=True, index=False)\n",
    "\n",
    "# Find Subsections\n",
    "subsection_data = []\n",
    "sections = soup.find_all('div', class_='Section toc-destination rbox')\n",
    "\n",
    "i=0\n",
    "for section in sections:\n",
    "    a_tag = section.find('a')\n",
    "    if a_tag:\n",
    "        section_id = a_tag.attrs['id']\n",
    "    \n",
    "    #section_id = section.get('id')\n",
    "\n",
    "    # Find the sibling divs with class 'rbox Normal-Level' to get the subsections\n",
    "    subsection_siblings = section.find_next_siblings('div', class_='rbox Normal-Level')\n",
    "\n",
    "    # Loop through the subsections and extract text\n",
    "    for subsection in subsection_siblings:\n",
    "        # Get the text from the div that contains the subsection title and content\n",
    "        full_text = subsection.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        # Split and extract the title\n",
    "        # Split by space to extract the first part (like \"(a)\")\n",
    "        if full_text:\n",
    "            content = full_text\n",
    "\n",
    "            # Add the extracted title and content to the list\n",
    "            subsection_data.append({\n",
    "                'Section ID': section_id,\n",
    "                'Subsection ID': i,\n",
    "                'Subsection Content': content\n",
    "            })\n",
    "            i+=1\n",
    "\n",
    "# Convert to DataFrame\n",
    "subsection_data_df = pd.DataFrame(subsection_data)\n",
    "\n",
    "# Save subsection data to Week 6 directory\n",
    "subsection_data_df.to_csv(output_directory + \"subsection_data.csv\", header=True, index=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(subsection_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c84b8-1f41-4a74-9add-1ec251e9c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import html\n",
    "\n",
    "# Path to the HTML file\n",
    "file_path = '../../raw_data/lamunicipalcode.html'\n",
    "\n",
    "# Open and read the HTML file using lxml parser\n",
    "with open(file_path, 'r', encoding='utf-8') as municipal_code:\n",
    "    content = municipal_code.read()\n",
    "\n",
    "# Decode HTML entities (pre-cleaning step)\n",
    "content = html.unescape(content)  # Decodes things like &nbsp;, &quot;, etc.\n",
    "\n",
    "# Use lxml parser as a fallback for BeautifulSoup\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "# Helper function to dynamically clean up garbled characters\n",
    "def dynamic_clean_text(text):\n",
    "    replacements = {\n",
    "        'Â': '',  # Remove non-breaking space placeholder\n",
    "        'â€œ': '\"',  # Misinterpreted left double quotes\n",
    "        'â€': '\"',  # Misinterpreted right double quotes\n",
    "        'â€™': \"'\",  # Misinterpreted apostrophes\n",
    "        'â€“': '–',  # Misinterpreted en-dash\n",
    "        'â€¦': '...',  # Misinterpreted ellipsis\n",
    "        'œ': 'oe',  # Correct misinterpreted \"œ\"\n",
    "        '\\xa0': ' ',  # Non-breaking spaces turned to regular spaces\n",
    "        'â€œ': '\"',  # Misinterpreted quotes\n",
    "        'â€”': '-',  # Misinterpreted em-dash\n",
    "        '“': '\"',  # Smart left double quotes\n",
    "        '”': '\"',  # Smart right double quotes\n",
    "        '‘': \"'\",  # Smart left single quote\n",
    "        '’': \"'\",  # Smart right single quote\n",
    "    }\n",
    "    for key, value in replacements.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text\n",
    "\n",
    "# Find chapters\n",
    "chapters = soup.find_all('div', class_='rbox Chapter')\n",
    "chapter_data = []\n",
    "for chapter in chapters:\n",
    "    a_tag = chapter.find('a')\n",
    "    if a_tag:\n",
    "        chapter_id = a_tag.attrs['id']\n",
    "    chapter_text = dynamic_clean_text(chapter.get_text(separator=\" | \").strip())  # Apply cleaning here\n",
    "    \n",
    "    chapter_data.append({\n",
    "        'Chapter ID': chapter_id,\n",
    "        'Chapter Text': chapter_text\n",
    "    })\n",
    "\n",
    "chapter_data = pd.DataFrame.from_dict(chapter_data)\n",
    "\n",
    "# Save chapter data to the Week 6 directory\n",
    "output_directory = '../../intermediate_data/'\n",
    "chapter_data.to_csv(output_directory + \"chapter_data.csv\", header=True, index=False)\n",
    "\n",
    "# Find articles\n",
    "articles = soup.find_all('div', class_='Article toc-destination rbox')\n",
    "article_data = []\n",
    "for article in articles:\n",
    "    a_tag = article.find('a')\n",
    "    if a_tag:\n",
    "        article_id = a_tag.attrs['id']\n",
    "    article_text = dynamic_clean_text(article.get_text(separator=\" | \"))  # Apply cleaning here\n",
    "    article_data.append({\n",
    "        'Article ID': article_id,\n",
    "        'Article Text': article_text\n",
    "    })\n",
    "\n",
    "article_data = pd.DataFrame.from_dict(article_data)\n",
    "\n",
    "# Save article data to the Week 6 directory\n",
    "article_data.to_csv(output_directory + \"article_data.csv\", header=True, index=False)\n",
    "\n",
    "# Find sections\n",
    "sections = soup.find_all('div', class_='Section toc-destination rbox')\n",
    "section_data = []\n",
    "for section in sections:\n",
    "    a_tag = section.find('a')\n",
    "    if a_tag:\n",
    "        section_id = a_tag.attrs['id']\n",
    "    section_text = dynamic_clean_text(section.get_text(separator=\" | \"))  # Apply cleaning here\n",
    "    section_data.append({\n",
    "        'Section ID': section_id,\n",
    "        'Section Text': section_text\n",
    "    })\n",
    "    \n",
    "section_data = pd.DataFrame.from_dict(section_data)\n",
    "\n",
    "# Save section data to the Week 6 directory\n",
    "section_data.to_csv(output_directory + \"section_data.csv\", header=True, index=False)\n",
    "\n",
    "# Find subsections\n",
    "subsection_data = []\n",
    "i = 0\n",
    "for section in sections:\n",
    "    a_tag = section.find('a')\n",
    "    if a_tag:\n",
    "        section_id = a_tag.attrs['id']\n",
    "    \n",
    "    # Find the sibling divs with class 'rbox Normal-Level' to get the subsections\n",
    "    subsection_siblings = section.find_next_siblings('div', class_='rbox Normal-Level')\n",
    "\n",
    "    # Loop through the subsections and extract text\n",
    "    for subsection in subsection_siblings:\n",
    "        # Get the text from the div that contains the subsection title and content\n",
    "        full_text = dynamic_clean_text(subsection.get_text(separator=\" \", strip=True))  # Apply cleaning here\n",
    "\n",
    "        if full_text:\n",
    "            content = full_text\n",
    "\n",
    "            # Add the extracted title and content to the list\n",
    "            subsection_data.append({\n",
    "                'Section ID': section_id,\n",
    "                'Subsection ID': i,\n",
    "                'Subsection Content': content\n",
    "            })\n",
    "            i += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "subsection_data_df = pd.DataFrame(subsection_data)\n",
    "\n",
    "# Save subsection data to the Week 6 directory\n",
    "subsection_data_df.to_csv(output_directory + \"subsection_data.csv\", header=True, index=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(subsection_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9429152-a7c4-4c3b-80ad-e8e2838d128d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Section ID  Subsection ID  \\\n",
      "0        JD_11.00.              0   \n",
      "1        JD_11.00.              1   \n",
      "2        JD_11.00.              2   \n",
      "3        JD_11.00.              3   \n",
      "4        JD_11.00.              4   \n",
      "...            ...            ...   \n",
      "40861  JD_200.126.          40861   \n",
      "40862  JD_200.126.          40862   \n",
      "40863  JD_200.127.          40863   \n",
      "40864  JD_200.127.          40864   \n",
      "40865  JD_200.201.          40865   \n",
      "\n",
      "                                      Subsection Content  \n",
      "0           (Amended by Ord. No. 175,676, Eff. 1/11/04.)  \n",
      "1      (a) Short Title. Reference to Code in Prosecut...  \n",
      "2      (b) Existing Law Continued. The provisions of ...  \n",
      "3      (c) Construction. The provisions of this Code ...  \n",
      "4      (d) Effect of Code on Past Actions and Obligat...  \n",
      "...                                                  ...  \n",
      "40861     (Renumbered by Ord. No. 187,455, Eff. 4/1/22.)  \n",
      "40862  Nothing in this article shall be interpreted s...  \n",
      "40863     (Renumbered by Ord. No. 187,455, Eff. 4/1/22.)  \n",
      "40864  This article shall sunset upon the lifting of ...  \n",
      "40865  The Los Angeles Housing Department and Departm...  \n",
      "\n",
      "[40866 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import html\n",
    "\n",
    "# Path to the HTML file\n",
    "file_path = '../../raw_data/lamunicipalcode.html'\n",
    "\n",
    "# Open and read the HTML file using lxml parser\n",
    "with open(file_path, 'r', encoding='utf-8') as municipal_code:\n",
    "    content = municipal_code.read()\n",
    "\n",
    "# Decode HTML entities (pre-cleaning step)\n",
    "content = html.unescape(content)  # Decodes things like &nbsp;, &quot;, etc.\n",
    "\n",
    "# Use lxml parser as a fallback for BeautifulSoup\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "# Helper function to dynamically clean up garbled characters and strip unnecessary formatting\n",
    "def dynamic_clean_text(text):\n",
    "    # Replace all &nbsp; with normal space\n",
    "    text = text.replace(u'\\xa0', ' ')  # Replace non-breaking spaces with regular spaces\n",
    "    \n",
    "    # Remove excessive spaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Further cleaning for misinterpreted characters\n",
    "    replacements = {\n",
    "        'Â': '',  # Remove non-breaking space placeholder\n",
    "        'â€™': \"'\",  # Misinterpreted apostrophes\n",
    "        'â€œ': '\"',  # Misinterpreted left double quotes\n",
    "        'â€': '\"',  # Misinterpreted right double quotes\n",
    "        'â€“': '–',  # Misinterpreted en-dash\n",
    "        'â€¦': '...',  # Misinterpreted ellipsis\n",
    "        'œ': 'oe',  # Correct misinterpreted \"œ\"\n",
    "        '“': '\"',  # Smart left double quotes\n",
    "        '”': '\"',  # Smart right double quotes\n",
    "        '‘': \"'\",  # Smart left single quote\n",
    "        '’': \"'\",  # Smart right single quote\n",
    "    }\n",
    "    for key, value in replacements.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text.strip()\n",
    "\n",
    "# Find chapters\n",
    "chapters = soup.find_all('div', class_='rbox Chapter')\n",
    "chapter_data = []\n",
    "for chapter in chapters:\n",
    "    a_tag = chapter.find('a')\n",
    "    if a_tag:\n",
    "        chapter_id = a_tag.attrs['id']\n",
    "    chapter_text = dynamic_clean_text(chapter.get_text(separator=\" | \").strip())  # Apply cleaning here\n",
    "    \n",
    "    chapter_data.append({\n",
    "        'Chapter ID': chapter_id,\n",
    "        'Chapter Text': chapter_text\n",
    "    })\n",
    "\n",
    "chapter_data = pd.DataFrame.from_dict(chapter_data)\n",
    "\n",
    "# Save chapter data to the Week 6 directory\n",
    "output_directory = '../../intermediate_data/'\n",
    "chapter_data.to_csv(output_directory + \"chapter_data.csv\", header=True, index=False)\n",
    "\n",
    "# Find articles\n",
    "articles = soup.find_all('div', class_='Article toc-destination rbox')\n",
    "article_data = []\n",
    "for article in articles:\n",
    "    a_tag = article.find('a')\n",
    "    if a_tag:\n",
    "        article_id = a_tag.attrs['id']\n",
    "    article_text = dynamic_clean_text(article.get_text(separator=\" | \"))  # Apply cleaning here\n",
    "    article_data.append({\n",
    "        'Article ID': article_id,\n",
    "        'Article Text': article_text\n",
    "    })\n",
    "\n",
    "article_data = pd.DataFrame.from_dict(article_data)\n",
    "\n",
    "# Save article data to the Week 6 directory\n",
    "article_data.to_csv(output_directory + \"article_data.csv\", header=True, index=False)\n",
    "\n",
    "# Find sections\n",
    "sections = soup.find_all('div', class_='Section toc-destination rbox')\n",
    "section_data = []\n",
    "for section in sections:\n",
    "    a_tag = section.find('a')\n",
    "    if a_tag:\n",
    "        section_id = a_tag.attrs['id']\n",
    "    section_text = dynamic_clean_text(section.get_text(separator=\" | \"))  # Apply cleaning here\n",
    "    section_data.append({\n",
    "        'Section ID': section_id,\n",
    "        'Section Text': section_text\n",
    "    })\n",
    "    \n",
    "section_data = pd.DataFrame.from_dict(section_data)\n",
    "\n",
    "# Save section data to the Week 6 directory\n",
    "section_data.to_csv(output_directory + \"section_data.csv\", header=True, index=False)\n",
    "\n",
    "# Find subsections\n",
    "subsection_data = []\n",
    "i = 0\n",
    "for section in sections:\n",
    "    a_tag = section.find('a')\n",
    "    if a_tag:\n",
    "        section_id = a_tag.attrs['id']\n",
    "    \n",
    "    # Find the sibling divs with class 'rbox Normal-Level' to get the subsections\n",
    "    subsection_siblings = section.find_next_siblings('div', class_='rbox Normal-Level')\n",
    "\n",
    "    # Loop through the subsections and extract text\n",
    "    for subsection in subsection_siblings:\n",
    "        # Get the text from the div that contains the subsection title and content\n",
    "        full_text = dynamic_clean_text(subsection.get_text(separator=\" \", strip=True))  # Apply cleaning here\n",
    "\n",
    "        if full_text:\n",
    "            content = full_text\n",
    "\n",
    "            # Add the extracted title and content to the list\n",
    "            subsection_data.append({\n",
    "                'Section ID': section_id,\n",
    "                'Subsection ID': i,\n",
    "                'Subsection Content': content\n",
    "            })\n",
    "            i += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "subsection_data_df = pd.DataFrame(subsection_data)\n",
    "\n",
    "# Save subsection data to the Week 6 directory\n",
    "subsection_data_df.to_csv(output_directory + \"subsection_data.csv\", header=True, index=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(subsection_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fad7f6-a1d3-4ccc-81a4-c326fae9a4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

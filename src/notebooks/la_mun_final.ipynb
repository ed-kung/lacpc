{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4179f3ae-4172-4063-a457-cccb854ed145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import html\n",
    "\n",
    "# Path to the HTML file\n",
    "file_path = '../../raw_data/lamunicipalcode.html'\n",
    "\n",
    "# Open and read the HTML file using lxml parser\n",
    "with open(file_path, 'r', encoding='utf-8') as municipal_code:\n",
    "    content = municipal_code.read()\n",
    "\n",
    "# Decode HTML entities (pre-cleaning step)\n",
    "content = html.unescape(content)  # Decodes things like &nbsp;, &quot;, etc.\n",
    "\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "# Helper function to dynamically clean up garbled characters and strip unnecessary formatting\n",
    "def dynamic_clean_text(text):\n",
    "    # Replace all &nbsp; with normal space\n",
    "    text = text.replace(u'\\xa0', ' ')  # Replace non-breaking spaces with regular spaces\n",
    "    \n",
    "    # Remove excessive spaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Further cleaning for misinterpreted characters\n",
    "    replacements = {\n",
    "        'Â': '',  # Remove non-breaking space placeholder\n",
    "        'â€™': \"'\",  # Misinterpreted apostrophes\n",
    "        'â€œ': '\"',  # Misinterpreted left double quotes\n",
    "        'â€': '\"',  # Misinterpreted right double quotes\n",
    "        'â€“': '–',  # Misinterpreted en-dash\n",
    "        'â€¦': '...',  # Misinterpreted ellipsis\n",
    "        'œ': 'oe',  # Correct misinterpreted \"œ\"\n",
    "        '“': '\"',  # Smart left double quotes\n",
    "        '”': '\"',  # Smart right double quotes\n",
    "        '‘': \"'\",  # Smart left single quote\n",
    "        '’': \"'\",  # Smart right single quote\n",
    "    }\n",
    "    for key, value in replacements.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text.strip()\n",
    "\n",
    "# Find chapters\n",
    "chapters = soup.find_all('div', class_='rbox Chapter')\n",
    "chapter_data = []\n",
    "for chapter in chapters:\n",
    "    a_tag = chapter.find('a')\n",
    "    if a_tag:\n",
    "        chapter_id = a_tag.attrs['id']\n",
    "    chapter_text = dynamic_clean_text(chapter.get_text(separator=\" | \").strip())  # Apply cleaning here\n",
    "    \n",
    "    chapter_data.append({\n",
    "        'Chapter ID': chapter_id,\n",
    "        'Chapter Text': chapter_text\n",
    "    })\n",
    "\n",
    "chapter_data = pd.DataFrame.from_dict(chapter_data)\n",
    "\n",
    "# Save chapter data to the Week 6 directory\n",
    "output_directory = '../../intermediate_data/'\n",
    "chapter_data.to_csv(output_directory + \"chapter_data.csv\", header=True, index=False)\n",
    "\n",
    "# Find articles\n",
    "articles = soup.find_all('div', class_='Article toc-destination rbox')\n",
    "article_data = []\n",
    "for article in articles:\n",
    "    a_tag = article.find('a')\n",
    "    if a_tag:\n",
    "        article_id = a_tag.attrs['id']\n",
    "    article_text = dynamic_clean_text(article.get_text(separator=\" | \"))  # Apply cleaning here\n",
    "    article_data.append({\n",
    "        'Article ID': article_id,\n",
    "        'Article Text': article_text\n",
    "    })\n",
    "\n",
    "article_data = pd.DataFrame.from_dict(article_data)\n",
    "\n",
    "# Save article data to the Week 6 directory\n",
    "article_data.to_csv(output_directory + \"article_data.csv\", header=True, index=False)\n",
    "\n",
    "# Find sections and assign the correct Article ID and Chapter ID\n",
    "sections = soup.find_all('div', class_='Section toc-destination rbox')\n",
    "section_data = []\n",
    "for section in sections:\n",
    "    a_tag = section.find('a')\n",
    "    if a_tag:\n",
    "        section_id = a_tag.attrs['id']\n",
    "    \n",
    "    # Find the closest preceding article to assign the correct Article ID\n",
    "    parent_article = section.find_previous('div', class_='Article toc-destination rbox')\n",
    "    if parent_article:\n",
    "        article_id = parent_article.find('a').attrs['id']\n",
    "    \n",
    "    # Find the closest preceding chapter to assign the correct Chapter ID\n",
    "    parent_chapter = section.find_previous('div', class_='rbox Chapter')\n",
    "    if parent_chapter:\n",
    "        chapter_id = parent_chapter.find('a').attrs['id']\n",
    "    \n",
    "    section_text = dynamic_clean_text(section.get_text(separator=\" | \"))  # Apply cleaning here\n",
    "    section_data.append({\n",
    "        'Chapter ID': chapter_id,     \n",
    "        'Article ID': article_id,     \n",
    "        'Section ID': section_id,     \n",
    "        'Section Text': section_text  \n",
    "    })\n",
    "\n",
    "section_data = pd.DataFrame.from_dict(section_data)\n",
    "\n",
    "# Save section data to the Week 6 directory\n",
    "section_data.to_csv(output_directory + \"section_data.csv\", header=True, index=False)\n",
    "\n",
    "# Find subsections\n",
    "subsection_data = []\n",
    "i = 0\n",
    "for section in sections:\n",
    "    a_tag = section.find('a')\n",
    "    if a_tag:\n",
    "        section_id = a_tag.attrs['id']\n",
    "\n",
    "    # Find the closest preceding article for the subsection\n",
    "    parent_article = section.find_previous('div', class_='Article toc-destination rbox')\n",
    "    if parent_article:\n",
    "        article_id = parent_article.find('a').attrs['id']\n",
    "\n",
    "    # Find the closest preceding chapter for the subsection\n",
    "    parent_chapter = section.find_previous('div', class_='rbox Chapter')\n",
    "    if parent_chapter:\n",
    "        chapter_id = parent_chapter.find('a').attrs['id']\n",
    "\n",
    "    # Find the sibling divs with class 'rbox Normal-Level' to get the subsections\n",
    "    subsection_siblings = section.find_next_siblings('div', class_='rbox Normal-Level')\n",
    "\n",
    "    # Loop through the subsections and extract text\n",
    "    for subsection in subsection_siblings:\n",
    "        # Get the text from the div that contains the subsection title and content\n",
    "        full_text = dynamic_clean_text(subsection.get_text(separator=\" \", strip=True))  # Apply cleaning here\n",
    "\n",
    "        if full_text:\n",
    "            content = full_text\n",
    "\n",
    "            # Add the extracted title and content to the list\n",
    "            subsection_data.append({\n",
    "                'Chapter ID': chapter_id,      \n",
    "                'Article ID': article_id,      \n",
    "                'Section ID': section_id,      \n",
    "                'Subsection Content': content  \n",
    "            })\n",
    "            i += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "subsection_data_df = pd.DataFrame(subsection_data)\n",
    "\n",
    "# Save subsection data to the Week 6 directory\n",
    "subsection_data_df.to_csv(output_directory + \"subsection_data.csv\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea4540-6333-41ca-9334-a31eab96c3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc294f0-ff31-46a4-9869-3df2f6eaeb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "from unidecode import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "with open('../../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "MUNICIPALITY = \"City of Los Angeles\"\n",
    "DOCUMENT_NAME = \"City of Los Angeles Municipal Code\"\n",
    "FILENAME = \"../../raw_data/lamunicipalcode.html\"\n",
    "DOCID = \"lamunicipalcode\"\n",
    "\n",
    "CHUNK_SIZE = config['CHUNK_SIZE']\n",
    "CHUNK_OVERLAP = config['CHUNK_OVERLAP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0010432f-ae62-4cdd-93d2-5596fe25986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the HTML file\n",
    "\n",
    "with open(FILENAME, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "soup = BeautifulSoup(content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be866e5e-3855-45c9-861a-56330457020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data into chapters, articles, sections, and passages\n",
    "\n",
    "divs = soup.find_all('div', class_='rbox')\n",
    "\n",
    "chapter_data = []\n",
    "article_data = []\n",
    "section_data = []\n",
    "passage_data = []\n",
    "\n",
    "chapter_id = ''\n",
    "article_id = ''\n",
    "section_id = ''\n",
    "curr_text = ''\n",
    "\n",
    "for div in divs:\n",
    "    class_ = div.attrs.get('class')\n",
    "    is_chapter = ('Chapter' in class_) and div.find('a')\n",
    "    is_article = ('Article' in class_) and div.find('a')\n",
    "    is_section = ('Section' in class_) and div.find('a')\n",
    "    \n",
    "    if is_chapter or is_article or is_section:\n",
    "        passage_data.append({\n",
    "            'id': DOCID + '_' + div_id,\n",
    "            'doc_id': DOCID, \n",
    "            'chapter_id': chapter_id, \n",
    "            'article_id': article_id,\n",
    "            'section_id': section_id,\n",
    "            'passage_id': div_id,\n",
    "            'item_type': 'passage',\n",
    "            'text': curr_text\n",
    "        })\n",
    "        curr_text = ''\n",
    "    \n",
    "    div_id = div.attrs.get('id')\n",
    "    text = div.get_text('\\n').replace(u'\\xa0',' ')\n",
    "    text = re.sub(r'\\s+',' ',text).strip()\n",
    "    text = unidecode(text)\n",
    "    \n",
    "    if is_chapter:\n",
    "        chapter_id = div.find('a').attrs.get('id')\n",
    "        article_id = ''\n",
    "        section_id = ''\n",
    "        chapter_data.append({\n",
    "            'id': DOCID + '_' + chapter_id,\n",
    "            'doc_id': DOCID,\n",
    "            'chapter_id': chapter_id,\n",
    "            'item_type': 'chapter_title',\n",
    "            'text': text\n",
    "        })\n",
    "    elif is_article:\n",
    "        article_id = div.find('a').attrs.get('id')\n",
    "        section_id = ''\n",
    "        article_data.append({\n",
    "            'id': DOCID + '_' + article_id,\n",
    "            'doc_id': DOCID,\n",
    "            'chapter_id': chapter_id,\n",
    "            'article_id': article_id,\n",
    "            'item_type': 'article_title',\n",
    "            'text': text\n",
    "        })\n",
    "    elif is_section:\n",
    "        section_id = div.find('a').attrs.get('id')\n",
    "        section_data.append({\n",
    "            'id': DOCID + '_' + section_id,\n",
    "            'doc_id': DOCID,\n",
    "            'chapter_id': chapter_id,\n",
    "            'article_id': article_id,\n",
    "            'section_id': section_id,\n",
    "            'item_type': 'section_title',\n",
    "            'text': text\n",
    "        })\n",
    "    else:\n",
    "        curr_text += text + ' '\n",
    "\n",
    "passage_data.append({\n",
    "    'id': DOCID + '_' + div_id,\n",
    "    'doc_id': DOCID, \n",
    "    'chapter_id': chapter_id, \n",
    "    'article_id': article_id,\n",
    "    'section_id': section_id,\n",
    "    'passage_id': div_id,\n",
    "    'item_type': 'passage',\n",
    "    'text': curr_text\n",
    "})\n",
    "\n",
    "chapter_data = pd.DataFrame.from_dict(chapter_data)\n",
    "article_data = pd.DataFrame.from_dict(article_data)\n",
    "section_data = pd.DataFrame.from_dict(section_data)\n",
    "passage_data = pd.DataFrame.from_dict(passage_data)\n",
    "\n",
    "chapter_data.to_csv(\"../../intermediate_data/los_angeles_chapter_data.csv\", header=True, index=False)\n",
    "article_data.to_csv(\"../../intermediate_data/los_angeles_article_data.csv\", header=True, index=False)\n",
    "section_data.to_csv(\"../../intermediate_data/los_angeles_section_data.csv\", header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b361ef-4e6b-469e-910c-0a8a198b56ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0... 1000... 2000... 3000... 4000... 5000... "
     ]
    }
   ],
   "source": [
    "# Chunk the passages\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "chunk_data = []\n",
    "for idx, row in passage_data.iterrows():\n",
    "    if (idx%1000==0):\n",
    "        print(f\"{idx}... \", end='')\n",
    "    text = row['text']\n",
    "    texts = text_splitter.create_documents([text])\n",
    "    chunk_id = 0\n",
    "    for tx in texts:\n",
    "        new_row = dict(row).copy()\n",
    "        new_row.pop('text')\n",
    "        new_row.pop('item_type')\n",
    "        new_row['id'] = new_row['id'] + f'_{chunk_id}'\n",
    "        new_row['chunk_id'] = chunk_id\n",
    "        new_row['item_type'] = 'passage_chunk'\n",
    "        new_row['text'] = tx.page_content\n",
    "        chunk_data.append(new_row)\n",
    "        chunk_id += 1\n",
    "        \n",
    "chunk_data = pd.DataFrame.from_dict(chunk_data)\n",
    "\n",
    "chunk_data.to_csv(\"../../intermediate_data/los_angeles_passage_chunk_data.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
